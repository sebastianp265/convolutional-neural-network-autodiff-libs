{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8510d899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Code/1DI2153/MLP`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ee995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2\n",
    "X_train = load(\"../data/imdb_dataset_prepared.jld2\", \"X_train\")\n",
    "y_train = load(\"../data/imdb_dataset_prepared.jld2\", \"y_train\")\n",
    "X_test = load(\"../data/imdb_dataset_prepared.jld2\", \"X_test\")\n",
    "y_test = load(\"../data/imdb_dataset_prepared.jld2\", \"y_test\")\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e70240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (7.93s) \tTrain: (l: 0.64, a: 0.80) \tTest: (l: 0.57, a: 0.85)\n",
      "Epoch: 2 (1.15s) \tTrain: (l: 0.45, a: 0.92) \tTest: (l: 0.43, a: 0.86)\n",
      "Epoch: 3 (1.13s) \tTrain: (l: 0.29, a: 0.94) \tTest: (l: 0.36, a: 0.87)\n",
      "Epoch: 4 (1.02s) \tTrain: (l: 0.20, a: 0.96) \tTest: (l: 0.33, a: 0.87)\n",
      "Epoch: 5 (1.01s) \tTrain: (l: 0.14, a: 0.98) \tTest: (l: 0.32, a: 0.87)\n"
     ]
    }
   ],
   "source": [
    "using Flux, Printf, Statistics\n",
    "\n",
    "dataset = Flux.DataLoader((X_train, y_train), batchsize=64, shuffle=true)\n",
    "\n",
    "model = Chain(\n",
    "    Dense(size(X_train, 1), 32, relu),\n",
    "    Dense(32, 1, sigmoid)\n",
    ")\n",
    "\n",
    "loss(m, x, y) = Flux.Losses.binarycrossentropy(m(x), y)\n",
    "accuracy(m, x, y) = mean((m(x) .> 0.5) .== (y .> 0.5))\n",
    "\n",
    "opt = Flux.setup(Adam(), model)\n",
    "epochs = 5\n",
    "for epoch in 1:epochs\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    t = @elapsed begin\n",
    "        for (x, y) in dataset\n",
    "            grads = Flux.gradient(model) do m\n",
    "                l = loss(m, x, y)\n",
    "                total_loss += l\n",
    "                total_acc += accuracy(m, x, y)\n",
    "                return l\n",
    "            end\n",
    "            Optimisers.update!(opt, model, grads[1])\n",
    "            num_samples += 1\n",
    "        end\n",
    "\n",
    "        train_loss = total_loss / num_samples\n",
    "        train_acc = total_acc / num_samples\n",
    "\n",
    "        test_acc = accuracy(model, X_test, y_test)\n",
    "        test_loss = loss(model, X_test, y_test)\n",
    "    end\n",
    "\n",
    "    println(@sprintf(\"Epoch: %d (%.2fs) \\tTrain: (l: %.2f, a: %.2f) \\tTest: (l: %.2f, a: %.2f)\", \n",
    "        epoch, t, train_loss, train_acc, test_loss, test_acc))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65027a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
